{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccaa408",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "Transformers first appeared around 2017 they are an attention based mdoel. The idea is that not all regions of the data are important and thus the model can pay attention to certain spaces within the dataset. \n",
    "\n",
    "- The first step is to compute the relvancy scores for between each vector. \n",
    "- The second step is to compute the sum of all vectors weighted by relevancy score.\n",
    "- Then repeat for each vector in the sequence. \n",
    "\n",
    "This can also be thought about in a query key value model. This is used by search engines to find relevant documents. \n",
    "\n",
    "The output of the seatch is outputs = sum(inputs * pairwise_score(inputs, inputs)) this can be re written with value, query , key instead of the inputs. In   practice all the terms are the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c56692",
   "metadata": {},
   "source": [
    "## Multihead attention\n",
    "An expansion on attention where we conactenate three ir nire seperate attention heads tyo form a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c206c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 19:17:20.971176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-07 19:17:21.756366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim = embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self,inputs,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask = mask)\n",
    "        proj_input = self.layernorm1(inputs+attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\":self.embed_dim, \"dense_dim\": self.dense_dim, \"num_heads\":self.num_heads})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca154ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 19:17:22.422969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.444386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.444571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.445567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.445726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.445867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.516264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.516448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.516596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-07 19:17:22.516714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1132 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, None, 256)         543776    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5664033 (21.61 MB)\n",
      "Trainable params: 5664033 (21.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "num_heads = 2\n",
    "embed_dim = 256\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype = \"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1362b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vec = layers.TextVectorization(max_tokens=max_tokens, output_mode = \"int\", output_sequence_length=max_length)\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"/home/kaspar/Documents/code/data/aclImdb/train\", batch_size=32)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"/home/kaspar/Documents/code/data/aclImdb/val\",batch_size=32)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"/home/kaspar/Documents/code/data/aclImdb/test\",batch_size=32)\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d89e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 51s 81ms/step - loss: 0.3409 - accuracy: 0.8536 - val_loss: 0.3242 - val_accuracy: 0.8556\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3158 - accuracy: 0.8619\n",
      "Test acc: 0.862\n"
     ]
    }
   ],
   "source": [
    "callbacks = keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\")\n",
    "model.fit(int_train_ds.cache(), validation_data=int_val_ds.cache(), epochs=1, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"transformer_encoder.keras\", custom_objects = {\"TransformerEncoder\":TransformerEncoder})\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba55af6",
   "metadata": {},
   "source": [
    "Note just giving it one session as there is little point training this thing yet since it still has no infomration about sequential information and thus will only ever get 87.5% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bf99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim = input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim = output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta = 1)\n",
    "        embed_tokens = self.token_embeddings(inputs)\n",
    "        embed_positions = self.position_embeddings(positions)\n",
    "        return embed_tokens + embed_positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask = None):\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"input_dim\":self.input_dim, \"sequence_length\": self.sequence_length, \"output_dim\":self.output_dim})\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3faa6940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, None, 256)         5273600   \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817633 (22.19 MB)\n",
      "Trainable params: 5817633 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 72s 111ms/step - loss: 0.5382 - accuracy: 0.7354 - val_loss: 0.4587 - val_accuracy: 0.7908\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.3010 - accuracy: 0.8742 - val_loss: 0.4079 - val_accuracy: 0.8386\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.2302 - accuracy: 0.9084 - val_loss: 0.3829 - val_accuracy: 0.8486\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.1866 - accuracy: 0.9295 - val_loss: 0.3728 - val_accuracy: 0.8734\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.1604 - accuracy: 0.9416 - val_loss: 0.4407 - val_accuracy: 0.8592\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.4536 - accuracy: 0.8537\n",
      "Test acc: 0.854\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "num_heads = 2\n",
    "embed_dim = 256\n",
    "dense_dim = 32\n",
    "sequence_length = 600\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype = \"int64\")\n",
    "x = PositionalEmbedding(sequence_length,vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\")\n",
    "model.fit(int_train_ds.cache(), validation_data=int_val_ds.cache(), epochs=5, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"full_transformer_encoder.keras\", custom_objects = {\"TransformerEncoder\":TransformerEncoder, \"PositionalEmbedding\":PositionalEmbedding})\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a61274",
   "metadata": {},
   "source": [
    "# Notes\n",
    "So this transformer is now the full model. It can operates a reasonably good model. However note that it doesnt perform as good as bag of words in this instance. That is because the sequence len gths are relatively short for which bag of words is adventageous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823c6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
