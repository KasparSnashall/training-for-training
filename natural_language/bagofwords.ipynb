{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "This example takes us through the first type of tokenisation, bag of words. Here we dont take into account the order of words but instead group them into snippets of N in length. If we have a 3Ngram then we create a set out of a sentance with single words, words in pairs, and words in triplets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    n_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-n_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code should be the same as in the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/train\", batch_size=32)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/val\",batch_size=32)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/test\",batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vec = TextVectorization(max_tokens = 20000, output_mode = \"multi_hot\")\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
