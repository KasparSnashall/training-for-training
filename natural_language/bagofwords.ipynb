{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "This example takes us through the first type of tokenisation, bag of words. Here we dont take into account the order of words but instead group them into snippets of N in length. If we have a 3Ngram then we create a set out of a sentance with single words, words in pairs, and words in triplets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\kaspa\\\\Documents\\\\Code\\\\AI_training\\\\training_data\\\\aclImdb_v1\\\\aclImdb\\\\val\\\\neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(train_dir\u001b[38;5;241m/\u001b[39mcategory)\n\u001b[0;32m     10\u001b[0m     random\u001b[38;5;241m.\u001b[39mRandom(\u001b[38;5;241m1337\u001b[39m)\u001b[38;5;241m.\u001b[39mshuffle(files)\n",
      "File \u001b[1;32mD:\\Program_Files\\Anaconda\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\kaspa\\\\Documents\\\\Code\\\\AI_training\\\\training_data\\\\aclImdb_v1\\\\aclImdb\\\\val\\\\neg'"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    n_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-n_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code should be the same as in the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/train\", batch_size=32)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/val\",batch_size=32)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/test\",batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vec = TextVectorization(max_tokens = 20000, output_mode = \"multi_hot\")\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=6):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 120006    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,013\n",
      "Trainable params: 120,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 33s 51ms/step - loss: 0.4951 - accuracy: 0.7802 - val_loss: 0.3637 - val_accuracy: 0.8774\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3833 - accuracy: 0.8575 - val_loss: 0.3369 - val_accuracy: 0.8796\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3492 - accuracy: 0.8684 - val_loss: 0.3340 - val_accuracy: 0.8702\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3426 - accuracy: 0.8702 - val_loss: 0.3492 - val_accuracy: 0.8638\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3414 - accuracy: 0.8705 - val_loss: 0.3686 - val_accuracy: 0.8620\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3347 - accuracy: 0.8754 - val_loss: 0.3855 - val_accuracy: 0.8596\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3248 - accuracy: 0.8770 - val_loss: 0.3916 - val_accuracy: 0.8552\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3206 - accuracy: 0.8857 - val_loss: 0.3988 - val_accuracy: 0.8612\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.3077 - accuracy: 0.8897 - val_loss: 0.4114 - val_accuracy: 0.8584\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.3040 - accuracy: 0.8941 - val_loss: 0.4289 - val_accuracy: 0.8570\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.3272 - accuracy: 0.8704\n",
      "Test acc: 0.870\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
    "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice to see how well even a simple binary 1gram does. This is a fairly complex task of figuring out which review is positive or negative. Even people who are learning a langague mught struggle at this without a year or so of learning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams with binary encoding\n",
    "Now onto using bag of words with some ability to interpret position. A bigram uses the pairs of words and so has a little more information to work with. I wonder how this all works into information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorisation = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"multi_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 120006    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,013\n",
      "Trainable params: 120,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 34s 53ms/step - loss: 0.4899 - accuracy: 0.7592 - val_loss: 0.3400 - val_accuracy: 0.8738\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3573 - accuracy: 0.8572 - val_loss: 0.3110 - val_accuracy: 0.8766\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.3253 - accuracy: 0.8765 - val_loss: 0.3161 - val_accuracy: 0.8760\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3219 - accuracy: 0.8742 - val_loss: 0.3302 - val_accuracy: 0.8722\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3215 - accuracy: 0.8759 - val_loss: 0.3432 - val_accuracy: 0.8654\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3254 - accuracy: 0.8762 - val_loss: 0.3681 - val_accuracy: 0.8656\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 27s 42ms/step - loss: 0.3255 - accuracy: 0.8756 - val_loss: 0.3652 - val_accuracy: 0.8594\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.3174 - accuracy: 0.8760 - val_loss: 0.3767 - val_accuracy: 0.8594\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3269 - accuracy: 0.8752 - val_loss: 0.3819 - val_accuracy: 0.8608\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.3231 - accuracy: 0.8760 - val_loss: 0.3857 - val_accuracy: 0.8514\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.3026 - accuracy: 0.8823\n",
      "Test acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazed at how quick that was to change to model setup to a 2gram. That TextVectorisation function is mean! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
