{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "This example takes us through the first type of tokenisation, bag of words. Here we dont take into account the order of words but instead group them into snippets of N in length. If we have a 3Ngram then we create a set out of a sentance with single words, words in pairs, and words in triplets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\kaspa\\\\Documents\\\\Code\\\\AI_training\\\\training_data\\\\aclImdb_v1\\\\aclImdb\\\\val\\\\neg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(train_dir\u001b[38;5;241m/\u001b[39mcategory)\n\u001b[0;32m     10\u001b[0m     random\u001b[38;5;241m.\u001b[39mRandom(\u001b[38;5;241m1337\u001b[39m)\u001b[38;5;241m.\u001b[39mshuffle(files)\n",
      "File \u001b[1;32mD:\\Program_Files\\Anaconda\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\kaspa\\\\Documents\\\\Code\\\\AI_training\\\\training_data\\\\aclImdb_v1\\\\aclImdb\\\\val\\\\neg'"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil, random # only run this once.\n",
    "\n",
    "base_dir = pathlib.Path(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    n_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-n_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code should be the same as in the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/train\", batch_size=32)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/val\",batch_size=32)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"C:/Users/kaspa/Documents/Code/AI_training/training_data/aclImdb_v1/aclImdb/test\",batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vec = TextVectorization(max_tokens = 20000, output_mode = \"multi_hot\")\n",
    "text_only_train_ds = train_ds.map(lambda x,y:x)\n",
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=6):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 120006    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,013\n",
      "Trainable params: 120,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 43s 66ms/step - loss: 0.5168 - accuracy: 0.7742 - val_loss: 0.3913 - val_accuracy: 0.8614\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3946 - accuracy: 0.8546 - val_loss: 0.3503 - val_accuracy: 0.8588\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3539 - accuracy: 0.8697 - val_loss: 0.3479 - val_accuracy: 0.8586\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3400 - accuracy: 0.8751 - val_loss: 0.3564 - val_accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3245 - accuracy: 0.8796 - val_loss: 0.3770 - val_accuracy: 0.8584\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3199 - accuracy: 0.8814 - val_loss: 0.3838 - val_accuracy: 0.8534\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.3206 - accuracy: 0.8834 - val_loss: 0.4123 - val_accuracy: 0.8536\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3139 - accuracy: 0.8891 - val_loss: 0.4265 - val_accuracy: 0.8564\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3116 - accuracy: 0.8855 - val_loss: 0.4264 - val_accuracy: 0.8472\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3134 - accuracy: 0.8863 - val_loss: 0.4355 - val_accuracy: 0.8444\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.3382 - accuracy: 0.8617\n",
      "Test acc: 0.862\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
    "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice to see how well even a simple binary 1gram does. This is a fairly complex task of figuring out which review is positive or negative. Even people who are learning a langague mught struggle at this without a year or so of learning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams with binary encoding\n",
    "Now onto using bag of words with some ability to interpret position. A bigram uses the pairs of words and so has a little more information to work with. I wonder how this all works into information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorisation = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"multi_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 120006    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,013\n",
      "Trainable params: 120,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 34s 53ms/step - loss: 0.4928 - accuracy: 0.7619 - val_loss: 0.3399 - val_accuracy: 0.8754\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3673 - accuracy: 0.8364 - val_loss: 0.3100 - val_accuracy: 0.8848\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3326 - accuracy: 0.8538 - val_loss: 0.3061 - val_accuracy: 0.8878\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3152 - accuracy: 0.8597 - val_loss: 0.3205 - val_accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3219 - accuracy: 0.8609 - val_loss: 0.3310 - val_accuracy: 0.8824\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3161 - accuracy: 0.8626 - val_loss: 0.3392 - val_accuracy: 0.8754\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3170 - accuracy: 0.8615 - val_loss: 0.3442 - val_accuracy: 0.8834\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3109 - accuracy: 0.8620 - val_loss: 0.3528 - val_accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3258 - accuracy: 0.8593 - val_loss: 0.3607 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3263 - accuracy: 0.8590 - val_loss: 0.3636 - val_accuracy: 0.8768\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.2970 - accuracy: 0.8863\n",
      "Test acc: 0.886\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazed at how quick that was to change to model setup to a 2gram. That TextVectorisation function is mean! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF enconding\n",
    "This encoding includes the ability to count the number of occurences of words as well. The example given in the book shows that this is not that effective for this task but we will give it a go anyway. TF = term frequnecy, IDF = in document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = TextVectorization(ngrams = 2, max_tokens=20000, output_mode=\"tf_idf\") # enable IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.6370 - accuracy: 0.6353 - val_loss: 0.5107 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.5518 - accuracy: 0.7265 - val_loss: 0.4582 - val_accuracy: 0.8356\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.5025 - accuracy: 0.7645 - val_loss: 0.4421 - val_accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.4829 - accuracy: 0.7771 - val_loss: 0.4329 - val_accuracy: 0.8394\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 28s 46ms/step - loss: 0.4451 - accuracy: 0.8110 - val_loss: 0.4191 - val_accuracy: 0.8436\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.4102 - accuracy: 0.8303 - val_loss: 0.3737 - val_accuracy: 0.8584\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 46ms/step - loss: 0.3872 - accuracy: 0.8346 - val_loss: 0.3782 - val_accuracy: 0.8482\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.3749 - accuracy: 0.8386 - val_loss: 0.4038 - val_accuracy: 0.8460\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3670 - accuracy: 0.8393 - val_loss: 0.3945 - val_accuracy: 0.8422\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.3732 - accuracy: 0.8368 - val_loss: 0.4010 - val_accuracy: 0.8362\n",
      "782/782 [==============================] - 32s 41ms/step - loss: 0.3606 - accuracy: 0.8564\n",
      "Test acc: 0.856\n"
     ]
    }
   ],
   "source": [
    "text_vec.adapt(text_only_train_ds)\n",
    "tfidf_train_ds = train_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "tfidf_val_ds = val_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "tfidf_test_ds = test_ds.map(lambda x,y: (text_vec(x),y), num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
    "model.fit(tfidf_train_ds.cache(), validation_data=tfidf_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(tfidf_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
